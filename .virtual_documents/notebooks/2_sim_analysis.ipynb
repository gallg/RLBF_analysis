


import sys
import os
base_path = ".."
sys.path.insert(0, base_path)
os.chdir(base_path)

from scipy.stats import t
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import itertools

%matplotlib inline


parameters = {
    'snr': [0.5, 1, 2, 3, 5],
    'n_bins': [10],
    'kernel_size': [20],
    'kernel_sigma': [0.5, 1.0, 2.0, 3.0, 4.0],
    'learning_rate': [0.05, 0.1, 0.3, 0.4, 0.8, 0.9],
    'temperature': [0.2],
    'min_temperature': [1e-5],
    'max_temperature': [1.0],
    'reduce_temperature': [False],
    'decay_rate': [0.001]
}

# load data;
data = pd.read_pickle("./data/hyperparameter_search.pkl")
print("Unique combinations of parameters", data.shape[0])








# set up variables;
unique_snr_values = data["snr"].unique()
unique_kernel_sigma_values = data["kernel_sigma"].unique()
unique_learning_rate_values = data["learning_rate"].unique()
unique_temperature_values = data["temperature"].unique()

# Generate combinations of snr and kernel_sigma
snr_kernel_combinations = list(itertools.product(
    unique_snr_values,
    unique_kernel_sigma_values
))

# Generate all combinations of snr, kernel_sigma, lr, and temp
lr_temp_combinations = list(itertools.product(
    unique_learning_rate_values,
    unique_temperature_values
))

num_rows = len(unique_snr_values)
num_cols = len(unique_kernel_sigma_values)
row_idx = 0
col_idx = 0

n_epochs = 100
epoch_length = list(range(0, n_epochs))
fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 10))

# loop through all combinations;
for idx, (snr, kernel_sigma) in enumerate(snr_kernel_combinations):
    # vars to save trials data;
    trial_summary = []
    trial_ci = []
    trial_lr_summary = []

    # get data for current parameters;
    sub_data = data[(data['snr'] == snr) & (data['kernel_sigma'] == kernel_sigma)]

    # generate subplot data;
    for epoch in epoch_length:
        trial_data = sub_data[sub_data["epochs"] == epoch]
        trial_summary.append(np.mean(trial_data["reward"]))
        trial_lr_summary.append(np.mean(trial_data["learning_rate"]))

        # get only data with best learning_rate;
        best_learning_rate = trial_data.loc[trial_data["reward"].idxmax(), ["learning_rate"]]
        trial_data = trial_data[trial_data["learning_rate"] == best_learning_rate.iloc[0]]

        # calculate the 95% confidence interval for reward;
        confidence_interval = t.interval(0.95, len(trial_data["reward"]) - 1,
                                         loc=np.mean(trial_data["reward"]),
                                         scale=np.std(trial_data["reward"]) / np.sqrt(len(trial_data["reward"])))
        trial_ci.append((confidence_interval[1] - confidence_interval[0]) / 2)

    # apply median smoothing to trial_summary using a rolling window;
    window_size = 2
    smoothed_summary = np.convolve(trial_summary, np.ones(window_size)/window_size, mode='same')

    # Fig1. Plot performance with the smoothed line; 
    axs[row_idx, col_idx].plot(epoch_length, smoothed_summary, label='Smoothed Mean Reward')

    # Fill the 95% confidence interval
    lower_bound = np.array(smoothed_summary) - np.array(trial_ci)
    upper_bound = np.array(smoothed_summary) + np.array(trial_ci)
    axs[row_idx, col_idx].fill_between(epoch_length, lower_bound, upper_bound, alpha=0.3, label='95% Confidence Interval')

    axs[row_idx, col_idx].set_xlabel("Trials")
    axs[row_idx, col_idx].set_ylabel("Reward")
    axs[row_idx, col_idx].set_title(f"SNR: {snr}, Kernel Sigma: {kernel_sigma}")
    # axs[row_idx, col_idx].legend()

    # correctly index subplots
    col_idx += 1
    if col_idx == num_cols:
        row_idx += 1
        col_idx = 0

# adjust layout;
plt.tight_layout()
plt.show()





# set up variables;
unique_snr_values = data["snr"].unique()
unique_kernel_sigma_values = data["kernel_sigma"].unique()
unique_learning_rate_values = data["learning_rate"].unique()
unique_temperature_values = data["temperature"].unique()

# Generate combinations of snr and kernel_sigma
snr_kernel_combinations = list(itertools.product(
    unique_snr_values,
    unique_kernel_sigma_values
))

# Generate all combinations of snr, kernel_sigma, lr, and temp
lr_temp_combinations = list(itertools.product(
    unique_learning_rate_values,
    unique_temperature_values
))

num_rows = len(unique_snr_values)
num_cols = len(unique_kernel_sigma_values)
row_idx = 0
col_idx = 0

n_epochs = 100
epoch_length = list(range(0, n_epochs))
fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 10))

# loop through all combinations;
for idx, (snr, kernel_sigma) in enumerate(snr_kernel_combinations):
    # vars to save trials data;
    trial_summary = []
    trial_ci = []
    trial_best_lr = []
    trial_best_temp = []

    # get data for current parameters;
    sub_data = data[(data['snr'] == snr) & (data['kernel_sigma'] == kernel_sigma)]

    # generate subplot data;
    for epoch in epoch_length:
        trial_data = sub_data[sub_data["epochs"] == epoch]
        trial_summary.append(np.mean(trial_data["optimum"]))

        # get only data with best learning_rate;
        best_learning_rate = trial_data.loc[trial_data["optimum"].idxmax(), ["learning_rate"]]
        trial_data = trial_data[trial_data["learning_rate"] == best_learning_rate.iloc[0]]

        # calculate the 95% confidence interval;
        confidence_interval = t.interval(0.95, len(trial_data["optimum"]) - 1,
                                         loc=np.mean(trial_data["optimum"]),
                                         scale=np.std(trial_data["optimum"]) / np.sqrt(len(trial_data["optimum"])))
        trial_ci.append((confidence_interval[1] - confidence_interval[0]) / 2)

    # Apply median smoothing to trial_summary using a rolling window
    window_size = 2
    smoothed_summary = np.convolve(trial_summary, np.ones(window_size)/window_size, mode='same')

    # Plot the smoothed line
    axs[row_idx, col_idx].plot(epoch_length, smoothed_summary, label='Smoothed Mean optimal Q-value')

    # Fill the 95% confidence interval
    lower_bound = np.array(smoothed_summary) - np.array(trial_ci)
    upper_bound = np.array(smoothed_summary) + np.array(trial_ci)
    axs[row_idx, col_idx].fill_between(epoch_length, lower_bound, upper_bound, alpha=0.3, label='95% Confidence Interval')

    axs[row_idx, col_idx].set_xlabel("Trials")
    axs[row_idx, col_idx].set_ylabel("optimal Q")
    axs[row_idx, col_idx].set_title(f"SNR: {snr}, Kernel Sigma: {kernel_sigma}")
    # axs[row_idx, col_idx].legend()

    # correctly index subplots
    col_idx += 1
    if col_idx == num_cols:
        row_idx += 1
        col_idx = 0

# adjust layout;
plt.tight_layout()
plt.show()





# set up variables;
unique_snr_values = data["snr"].unique()
unique_kernel_sigma_values = data["kernel_sigma"].unique()
unique_learning_rate_values = data["learning_rate"].unique()
unique_temperature_values = data["temperature"].unique()

# Generate combinations of snr and kernel_sigma
snr_kernel_combinations = list(itertools.product(
    unique_snr_values,
    unique_kernel_sigma_values
))

# Generate all combinations of snr, kernel_sigma, lr, and temp
lr_temp_combinations = list(itertools.product(
    unique_learning_rate_values,
    unique_temperature_values
))

num_rows = len(unique_snr_values)
num_cols = len(unique_kernel_sigma_values)
row_idx = 0
col_idx = 0

n_epochs = 100
epoch_length = list(range(0, n_epochs))
fig2, axs2 = plt.subplots(num_rows, num_cols, figsize=(20, 10))

# loop through all combinations;
for idx, (snr, kernel_sigma) in enumerate(snr_kernel_combinations):
    # vars to save trials data;
    trial_summary = []
    trial_best_lr = []
    trial_lr_summary = []

    # get data for current parameters;
    sub_data = data[(data['snr'] == snr) & (data['kernel_sigma'] == kernel_sigma)]

    # generate subplot data;
    for epoch in epoch_length:
        trial_data = sub_data[sub_data["epochs"] == epoch]
        trial_summary.append(np.mean(trial_data["reward"]))
        trial_lr_summary.append(np.mean(trial_data["learning_rate"]))

        # get only data with best learning_rate;
        best_learning_rate = trial_data.loc[trial_data["reward"].idxmax(), ["learning_rate"]]
        trial_data = trial_data[trial_data["learning_rate"] == best_learning_rate.iloc[0]]
        trial_best_lr.append(best_learning_rate)

    # apply median smoothing to trial_summary using a rolling window;
    window_size = 2
    smoothed_summary = np.convolve(trial_summary, np.ones(window_size)/window_size, mode='same')

    # Fig2. Plot learning rate with the smoothed line; 
    axs2[row_idx, col_idx].plot(epoch_length, trial_best_lr, label='Best LR')

    axs2[row_idx, col_idx].set_xlabel("Trials")
    axs2[row_idx, col_idx].set_ylabel("Learning Rate")
    axs2[row_idx, col_idx].set_title(f"SNR: {snr}, Kernel Sigma: {kernel_sigma}")

    # correctly index subplots
    col_idx += 1
    if col_idx == num_cols:
        row_idx += 1
        col_idx = 0

# adjust layout;
plt.tight_layout()
plt.show()








import numpy as np
import pandas as pd
import itertools
import matplotlib.pyplot as plt

# set up variables;
unique_snr_values = data["snr"].unique()
unique_kernel_sigma_values = data["kernel_sigma"].unique()
unique_temperature_values = data["temperature"].unique()

# Generate combinations of snr and kernel_sigma
snr_kernel_combinations = list(itertools.product(
    unique_snr_values,
    unique_kernel_sigma_values
))

num_rows = len(unique_snr_values)
num_cols = len(unique_kernel_sigma_values)
row_idx = 0
col_idx = 0

n_epochs = 100
epoch_length = list(range(0, n_epochs))
fig2, axs2 = plt.subplots(num_rows, num_cols, figsize=(20, 10))

# loop through all combinations;
for idx, (snr, kernel_sigma) in enumerate(snr_kernel_combinations):
    # vars to save trials data;
    trial_summary = []
    trial_best_temp = []
    trial_temp_summary = []

    # get data for current parameters;
    sub_data = data[(data['snr'] == snr) & (data['kernel_sigma'] == kernel_sigma)]

    # generate subplot data;
    for epoch in epoch_length:
        trial_data = sub_data[sub_data["epochs"] == epoch]
        trial_summary.append(np.mean(trial_data["reward"]))
        trial_temp_summary.append(np.mean(trial_data["temperature"]))

        # get only data with best temperature;
        best_temperature = trial_data.loc[trial_data["reward"].idxmax(), ["temperature"]]
        trial_data = trial_data[trial_data["temperature"] == best_temperature.iloc[0]]
        trial_best_temp.append(best_temperature)

    # apply median smoothing to trial_summary using a rolling window;
    window_size = 2
    smoothed_summary = np.convolve(trial_summary, np.ones(window_size)/window_size, mode='same')

    # Fig2. Plot temperature with the smoothed line; 
    axs2[row_idx, col_idx].plot(epoch_length, trial_best_temp, label='Best Temp')

    axs2[row_idx, col_idx].set_xlabel("Trials")
    axs2[row_idx, col_idx].set_ylabel("Temperature")
    axs2[row_idx, col_idx].set_title(f"SNR: {snr}, Kernel Sigma: {kernel_sigma}")

    # correctly index subplots
    col_idx += 1
    if col_idx == num_cols:
        row_idx += 1
        col_idx = 0

# adjust layout;
plt.tight_layout()
plt.show()






import os
os.chdir("/Users/giuseppe/Documents/Projects/RL_sim/")
from agents.utils import create_bins, discretize_observation
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns
import numpy as np
import pandas as pd
import json
import ast

def read_json_log(log):
    with open(log, "rb") as f:
        log = json.load(f)
    return log
    
def json_array_to_list(arr):
    return ast.literal_eval(arr)

def pad_to_max_length(arr, max_length):
    return np.pad(arr, (0, max_length - len(arr)), mode='constant', constant_values=0)

def score_datapoints(arr):
    mu = np.mean(arr)
    sigma = np.std(arr)
    return np.abs(arr - mu) < sigma

def calculate_snr(log):
    if isinstance(log, str):
        log = read_json_log(log)
    reward = np.array(json_array_to_list(log[-1]["reward"]))
    return reward.mean() / reward.std()

def plot_reward(log):
    log = read_json_log(log)
    reward = json_array_to_list(log[-1]["reward"])
    convergence = json_array_to_list(log[-1]["convergence"])
    size = range(len(reward))
    snr = np.float16(calculate_snr(log))
    df = pd.DataFrame(columns=["trial", "reward", "convergence"])
    df["trial"] = size
    df["reward"] = reward
    df["convergence"] = pad_to_max_length(convergence, len(reward))

    df = df[score_datapoints(df["reward"])]
    plt.figure(figsize=(10, 4))
    sns.regplot(x="trial", y="reward", data=df)
    plt.xlabel("Trials", fontsize=16)
    plt.ylabel("Reward", fontsize=16)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f"SNR: {snr}, Kernel Sigma: 4.0", fontsize=20)

    plt.figure(figsize=(10, 4))
    plt.plot(df["trial"], df["convergence"])
    plt.xlabel("Trials", fontsize=16)
    plt.ylabel("Convergence", fontsize=16)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)

    plt.show()

def get_q_table_max(log, table_size):
    maxima = []
    for table in log:
        q_table = json_array_to_list(table["q_table"])
        maxima.append(np.unravel_index(np.argmax(q_table), table_size))
    return maxima

def get_q_table_steps(log, num_bins):
    steps = []
    for data in log:
        contrast = data["contrast"]
        frequency = data["frequency"]
        bins = create_bins(num_bins)
        steps.append(discretize_observation([contrast, frequency], bins))
    return steps

def plot_q_table_steps(log, session, subject):
    log = read_json_log(log)
    last_table = np.array(json_array_to_list(log[-1]["q_table"])).reshape(10, 10)
    maxima = np.array(get_q_table_steps(log, 10)) + 1 # adjust discretization;
    steps = np.random.randint(0, len(maxima), len(maxima))

    plt.imshow(last_table)
    plt.xlabel("Frequency", fontsize=16)
    plt.ylabel("Contrast", fontsize=16)
    plt.colorbar().set_label("Q-value", fontsize=16)
    plt.plot(maxima[:, 1], maxima[:, 0], "--", alpha=0.4)
    plt.scatter(maxima[:, 1], maxima[:, 0], c=steps, cmap='plasma', alpha=0.5)
    plt.colorbar().set_label("Trial number", fontsize=16)
    plt.title(f"{subject}, fMRI - Session {session}", fontsize=20)
    plt.show()






for sub in ["sub-001", "sub-002", "sub-003", "sub-004", "sub-005", "sub-006", "sub-007", "sub-008", "sub-009", "sub-010"]:
    logs = [
        f"/Users/giuseppe/Documents/Projects/RL_sim/data/{sub}/data_1/log/log.json",
        f"/Users/giuseppe/Documents/Projects/RL_sim/data/{sub}/data_2/log/log.json"
    ]

    for log in logs:
        plot_reward(log)





for sub in ["sub-001", "sub-002", "sub-003", "sub-004", "sub-005", "sub-006", "sub-007", "sub-008", "sub-009", "sub-010"]:
    logs = [
        f"/Users/giuseppe/Documents/Projects/RL_sim/data/{sub}/data_1/log/log.json",
        f"/Users/giuseppe/Documents/Projects/RL_sim/data/{sub}/data_2/log/log.json"
    ]

    for idx, log in enumerate(logs):
        subject = log.split("/")[-4]
        plot_q_table_steps(log, idx + 1, subject)


for sub in ["sub-001", "sub-002", "sub-003"]:
    logs = [
        f"/Users/giuseppe/Documents/Projects/RL_sim/data/{sub}/data_1/log/log.json",
        f"/Users/giuseppe/Documents/Projects/RL_sim/data/{sub}/data_2/log/log.json"
    ]

    n_plot = 0
    fig, axs = plt.subplots(1, 2, figsize=(12, 4))

    for log in logs:

        log = read_json_log(log)
        reward = json_array_to_list(log[-1]["reward"])
        rolling_mean = pd.Series(reward).rolling(window=3).mean()

        # plot reward;
        axs[n_plot].set_title(f"{sub}, session: {n_plot}")
        axs[n_plot].plot(reward)
        axs[n_plot].plot(rolling_mean.to_numpy())
        axs[n_plot].set_xlabel("epochs")
        axs[n_plot].set_ylabel("reward")
        n_plot += 1


for sub in ["sub-001", "sub-002", "sub-003"]:
    logs = [
        f"/Users/giuseppe/Documents/Projects/RL_sim/data/{sub}/data_1/log/log.json",
        f"/Users/giuseppe/Documents/Projects/RL_sim/data/{sub}/data_2/log/log.json"
    ]

    n_plot = 0
    fig, axs = plt.subplots(1, 2, figsize=(12, 4))

    for log in logs:
        log = read_json_log(log)
        max_val = []
        min_val = []

        for idx in range(len(log)):
            table = np.array(json_array_to_list(log[idx]["q_table"]))
            min_val.append(np.min(table))
            max_val.append(np.max(table))


        # plot reward;
        axs[n_plot].set_title(f"{sub}, session: {n_plot}")
        axs[n_plot].plot(min_val)
        axs[n_plot].plot(max_val)
        axs[n_plot].set_xlabel("epochs")
        axs[n_plot].set_ylabel("reward")
        n_plot += 1


import seaborn as sns
import matplotlib.pyplot as plt

subjects = ["sub-001", "sub-002", "sub-003"]
datapoints = ["data_1", "data_2"]
logs = [f"/Users/giuseppe/Documents/Projects/RL_sim/data/{sub}/{data}/log/log.json" for sub in subjects for data in datapoints]

for log in logs:
    log = read_json_log(log)
    
    contrast = []
    frequency = []
    reward_log = []

    for idx in range(len(log)):
        contrast.append(json_array_to_list(log[idx]["last action"])[0])
        frequency.append(json_array_to_list(log[idx]["last action"])[1])

    reward = json_array_to_list(log[-1]["reward"])
    reward.insert(0, 0)

    data = pd.DataFrame({
        'Contrast': contrast,
        'Frequency': frequency,
        'Reward': reward
    })

    # Pairplot to visualize relationships
    sns.pairplot(data)
    plt.show()

    # Correlation matrix
    corr_matrix = data.corr()
    sns.heatmap(corr_matrix, annot=True)
    plt.show()
